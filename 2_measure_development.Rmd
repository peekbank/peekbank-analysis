---
title: "Trial analysis 2: reliability and data"
author: "Mike"
date: "2/19/2021"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: hide
---

```{r setup, echo = FALSE}
suppressPackageStartupMessages(library(here))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lme4))
suppressPackageStartupMessages(library(ggpmisc))
suppressPackageStartupMessages(library(ggrepel))
suppressPackageStartupMessages(library(ggthemes))
# remotes::install_github("jmgirard/agreement")
library(agreement)

# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, cache = TRUE, 
                      message=FALSE, warning=FALSE, error=FALSE)
options(dplyr.summarise.inform = FALSE)

load(file = here("cached_intermediates","1_d_trial.Rds"))
```

The goal of this portion of the analysis is to derive two measures: 1) accuracy and 2) reaction time from the underlying looking data. Along the way we will consider various decisions we could make about data cleaning and inclusion. 

Let's start by thinking about what the basic curve is and how to get out various measures. This curve is just averaging across every single administration timepoint - unweighted. The idea is just to give a sense for what the average probability function is that we're trying to measure. 

```{r eval=FALSE}
ggplot(d_trial, aes(x = t_norm, y = correct)) + 
  xlim(-2000,3500)+
  geom_smooth()
```

Seems like we want something that captures 1) the accuracy and 2) the rise coming soon after zero (RT).

We're going to try and develop measures for:

1. accuracy
2. baseline-corrected accuracy
3. RT

# Understanding ICCS

Here's the distribution of average looking across all trials.

```{r}
d_summary <- d_trial |>
  group_by(dataset_name, trial_id, dataset_id, subject_id, administration_id, 
           target_label) |>
  summarise(accuracy = mean(correct[t_norm > 500], na.rm=TRUE),
            prop_data = mean(!is.na(correct[t_norm > 500]))) |>
  filter(!is.na(accuracy))

ggplot(d_summary, aes(x = prop_data, y = accuracy)) +
  geom_point(alpha = .05)
```

We're going to use ICCs to measure reliability, using McGraw & Wong (1996). It seems like we want two-way random effects, no interaction (subjects and items are meaningful). This is type "2A." We want average agreement across units.

One big decision is whether to look across stimulus items, rather than across kids. Across stimulus items returns *much* higher values. This is in part because we typically have more kids than items, and kids are sort of like "raters." 

```{r}
get_icc <- function (x, column = "accuracy", object = "stimulus") {
  if (object == "stimulus") {
    iccs <- dim_icc(x, 
                    model = "2A", 
                    type = "agreement", 
                    unit = "average",
                    object = target_label, 
                    rater = administration_id,
                    trial = trial_id, 
                    score = {{column}}, 
                    bootstrap = 0)
  } else {
    iccs <- dim_icc(x, 
                    model = "2A", 
                    type = "agreement", 
                    unit = "average",
                    object = administration_id, 
                    rater = target_label,
                    trial = trial_id, 
                    score = {{column}}, 
                    bootstrap = 0)
  }
  
  return(iccs$Inter_ICC)
}
```

## Swingley and Aslin

Let's look at one dataset. Here are the stimulus and administration ICCs for Swingley & Aslin (2002).

```{r}
sa <- d_summary |> 
  filter(dataset_name == "swingley_aslin_2002")

sa_cleaned <- filter(sa, 
                     !(target_label %in% c("bird","duck","shoe","truck")))

ggplot(sa_cleaned, 
       aes(x = target_label, y = accuracy)) +
  geom_jitter(alpha = .5, width = .2) + 
  stat_summary(col = "red")
```

Now check ICCs.

```{r}
# disaggregated
get_icc(sa, object = "stimulus")
get_icc(sa, object = "administration")

# disaggregated and cleaned
get_icc(sa_cleaned, object = "stimulus")
get_icc(sa_cleaned, object = "administration")


# aggregated
sa_agg <- sa |>
  group_by(target_label, administration_id, trial_id) |>
  summarise(accuracy = mean(accuracy))
get_icc(sa_agg, object = "stimulus")
get_icc(sa_agg, object = "administration")
```

I don't understand the zero. Hypothesis - this is about not differentiating two different observations for each `target_label`. 

```{r}
dim_icc(sa, 
        model = "2A", 
        type = "agreement", 
        unit = "average",
        object = administration_id, 
        rater = target_label,
        trial = trial_id, 
        score = accuracy, 
        bootstrap = 1000)
```

```{r}
dim_icc(sa, 
        model = "2A", 
        type = "agreement", 
        unit = "average",
        object = target_label, 
        rater = administration_id,
        trial = trial_id, 
        score = accuracy, 
        bootstrap = 1000)
```

Last question: inter vs. intra-rater reliability. 

```{r}
dim_icc(sa, 
        model = "2A", 
        type = "agreement", 
        unit = "average",
        object = administration_id, 
        rater = target_label,
        trial = trial_id, 
        score = accuracy, 
        bootstrap = 0)
dim_icc(sa_agg, 
        model = "2A", 
        type = "agreement", 
        unit = "average",
        object = administration_id, 
        rater = target_label,
        score = accuracy, 
        bootstrap = 0)
```
We're not actually sure how the **intra**-rater reliabilities are computed when you have only one observation per rater (shouldn't they be zero then?). But we're pretty clear we want the **inter**-rater reliabilities.

Take-homes:
* can't have multiple observations without a disambiguating trial label
* average absolute inter-rater reliability is what we want
* averaging across multiple observations increases reliabilities


## Across datasets

Note that we need to remove NaNs to make the ICCs work. 

```{r}
iccs <- d_summary |>
  group_by(dataset_name) |> 
  nest() |>
  mutate(icc_stimulus_acc = unlist(map(data, ~get_icc(.x, object = "stimulus"))),
         icc_admin_acc = unlist(map(data, ~get_icc(.x, object = "administration")))) |>
  select(-data) |>
  unnest(cols = c())

knitr::kable(iccs, digits = 2)
```

OK, to summarize, we think we understand the ICCs. They are:

* higher for administrations in datasets with lots of items (e.g., adams marchman)
* higher for stimuli in big datasets with few items (e.g., attword)
* reliably not NaN or 0 because we solved missing data (can't have) and repeated trials (need to mark trial id) issues. 


# Measure 1: Accuracies

There's a lot of missing data and a lot of "zoners" (kids who look only at one side). Zoners are not just missing data kids.

```{r}
ggplot(filter(d_summary, prop_data > .75),
       aes(x = accuracy)) + 
  geom_histogram()
```

Should we exclude data to get a more reliable measure? Here are two different decisions we could optimize:

1. exclude zoners?
2. exclude based on prop data

Let's try to figure those out. 

Try to do this programmatically across all datasets.  

```{r, error=FALSE, message=FALSE, warning=FALSE}

icc_sim <- function (zoners_included, exclude_less_than, object) 
{
  df <- d_summary |>
    filter(prop_data > exclude_less_than)
  
  # drop zoners
  if (zoners_included == FALSE) { 
    df <- filter(df, accuracy > 0, accuracy < 1) 
  }
  
  # compute ICCs
  df |> 
    group_by(dataset_name) |> 
    nest() |>
    mutate(icc = unlist(map(data, ~get_icc(., "accuracy", object)))) |>
    select(-data) |>
    unnest(cols = c()) 
}

excl_params <- expand_grid(zoners_included = c(FALSE, TRUE),
                           exclude_less_than = seq(.1, .9, .1), 
                           object = c("stimulus", "administration")) |>
  mutate(icc = pmap(list(zoners_included, exclude_less_than, object), icc_sim)) |>
  unnest(col = icc)
```

Plot resulting ICCs.

```{r}
ggplot(excl_params,
       aes(x = exclude_less_than, y = icc, col = zoners_included)) + 
  geom_jitter(width = .01, alpha = .5) + 
  geom_smooth(method = "lm") + 
  facet_wrap(~object)
```

Looks to me like excluding zoners isn't a clear win (and a loss for stimulus ICC). Further, excluding on amount of data doesn't seem to gain us reliability. 

I find this surprising and want to double check from other perspectives. 

## Window size

These simulations use ICCs as a way to understand how we summarize accuracy data. In particular, we're going to look at how ICCs change as a function of window size. 

```{r warning=FALSE, message=FALSE, error=FALSE, eval=FALSE}
icc_window_sim <- function (t_start = -500, t_end = 4000, object) 
{
  print(paste(t_start, t_end))
  
  df <- d_trial |>
    filter(t_norm > t_start, t_norm < t_end) |>
    group_by(dataset_name, dataset_id, administration_id, target_label, trial_id) |>
    summarise(accuracy = mean(correct, na.rm=TRUE),
              prop_data = mean(!is.na(correct)))
  
  # compute ICCs
  df |> 
    group_by(dataset_name) |> 
    nest() |>
    mutate(icc = unlist(map(data, ~get_icc(., "accuracy", object)))) |>
    select(-data) |>
    unnest(cols = c()) 
}

acc_params <- expand_grid(t_start = seq(-1000,1500,500),
                          t_end = seq(2000,4000,500),
                          object = c("stimulus", "administration")) |>
  mutate(icc = pmap(list(t_start, t_end, object), icc_window_sim)) |>
  unnest(col = icc)

save(file = "cached_intermediates/2_acc_params.Rds", acc_params)
```

```{r}
load(file = "cached_intermediates/2_acc_params.Rds")

ggplot(acc_params, aes(x = t_start, y = icc, col = as.factor(t_end))) + 
  geom_jitter() + 
  facet_wrap(~object) + 
  geom_smooth(aes(group = as.factor(t_end)), se = FALSE)
```

Looks like for stimulus and administration you get consistent but modest gains if you take the longest window. BUT for stimuli, the early part of the trial adds reliability (probably because of bias due to stimulus-level preferences?). In contrast, for administrations, the early part of the trial is less informative. 500ms seems like a pretty good compromise. 

## Window size by age

Let's do one more simulation where we check if this result holds across two ages. We'll break down age into > 24 months and < 24 months, which roughly splits the dataset. There are `r  length(unique(d_trial$administration_id[d_trial$age < 24]))` younger kids and `r length(unique(d_trial$administration_id[d_trial$age >= 24]))` older kids. 


```{r warning=FALSE, message=FALSE, error=FALSE, eval=TRUE}
icc_window_sim <- function (t_start = 0, t_end = 4000, object) 
{
  df <- d_trial |>
    mutate(younger = age < 24) |>
    filter(t_norm > t_start, t_norm < t_end) |>
    group_by(dataset_name, dataset_id, younger, administration_id, 
             target_label, trial_id) |>
    summarise(accuracy = mean(correct, na.rm=TRUE),
              prop_data = mean(!is.na(correct)))
  
  # compute ICCs
  df |> 
    group_by(dataset_name, younger) |> 
    nest() |>
    mutate(icc = unlist(map(data, ~get_icc(., "accuracy", object)))) |>
    select(-data) |>
    unnest(cols = c()) 
}

acc_params_byage <- expand_grid(t_start = seq(0,1500,500),
                                t_end = seq(2000,4000,500),
                                object = c("stimulus", "administration")) |>
  mutate(icc = pmap(list(t_start, t_end, object), icc_window_sim)) |>
  unnest(col = icc)

save(file = "cached_intermediates/2_acc_params_byage.Rds", acc_params_byage)
```

Now plot. 

```{r}
load(file = "cached_intermediates/2_acc_params_byage.Rds")

ggplot(acc_params_byage, aes(x = t_start, y = icc, col = as.factor(t_end))) + 
  geom_jitter() + 
  facet_grid(younger~object) + 
  geom_smooth(aes(group = as.factor(t_end)), se = FALSE)
```

Here we see that the younger kids lose more reliability when the window is short, but otherwise the conclusions remain unchanged. 

# Measure 2: Baseline-corrected accuracy

For our next measure, we'll repeat the same exercise, but we'll baseline-corrected looking. Let's start by implementing baseline-correction in the simplest way possible. 

Zooming in on Adams-Marchman, since that had very high administration reliability. 

```{r}
qplot(data = filter(d_trial, 
                    dataset_name == "adams_marchman_2018"), 
      x = t_norm, geom = "histogram")
```

```{r}
am <- d_trial |>
  filter(dataset_name == "adams_marchman_2018") |>
  group_by(trial_id, subject_id, administration_id, 
           target_label) |>
  summarise(baseline = mean(correct[t_norm < 500], na.rm=TRUE),
            accuracy = mean(correct[t_norm > 500], na.rm=TRUE), 
            bc_accuracy = accuracy - baseline) |>
  filter(!is.na(accuracy), !is.na(bc_accuracy))

ggplot(am, 
       aes(x = accuracy)) +
  geom_histogram()

ggplot(am, 
       aes(x = bc_accuracy)) +
  geom_histogram()
```

Now check ICCs.

```{r}
# disaggregated
get_icc(am, column = "accuracy", object = "stimulus")
get_icc(am, column = "accuracy", object = "administration")
get_icc(am, column = "bc_accuracy", object = "stimulus")
get_icc(am, column = "bc_accuracy", object = "administration")
```

OK, so for this dataset it seems like within-trial baseline correction is **reducing** reliability for both stimuli and administrations. AM2018 still has relatively high reliability (in contrast to others). Let's try the SA dataset we were looking at before. 

```{r}
sa <- d_trial |>
  filter(dataset_name == "swingley_aslin_2002") |>
  group_by(trial_id, subject_id, administration_id, 
           target_label) |>
  summarise(baseline = mean(correct[t_norm < 500], na.rm=TRUE),
            accuracy = mean(correct[t_norm > 500], na.rm=TRUE), 
            bc_accuracy = accuracy - baseline) |>
  filter(!is.na(accuracy))

ggplot(am, 
       aes(x = accuracy)) +
  geom_histogram()

ggplot(am, 
       aes(x = bc_accuracy)) +
  geom_histogram()

# disaggregated
get_icc(sa, column = "accuracy", object = "stimulus")
get_icc(sa, column = "accuracy", object = "administration")
get_icc(sa, column = "bc_accuracy", object = "stimulus")
get_icc(sa, column = "bc_accuracy", object = "administration")
```

Weirdly it looks like the reverse is happening. Let's get more systematic. 

```{r}
d_summary <- d_trial |>
  group_by(dataset_name, trial_id, subject_id, administration_id, 
           target_label) |>
  summarise(baseline = mean(correct[t_norm < 500], na.rm=TRUE),
            accuracy = mean(correct[t_norm > 500], na.rm=TRUE), 
            bc_accuracy = accuracy - baseline, 
            target = sum(correct[t_norm > 500], na.rm=TRUE),
            target_baseline = sum(correct[t_norm < 500], na.rm=TRUE),
            distractor = sum(!correct[t_norm > 500], na.rm=TRUE), 
            distractor_baseline = sum(!correct[t_norm < 500], na.rm=TRUE), 
            elogit_baseline = log( (target_baseline + .5) / 
                                     (distractor_baseline + .5) ),
            elogit = log( (target + .5) / 
                            (distractor + .5) ), 
            elogit_bc = elogit - elogit_baseline) |>
  filter(!is.na(accuracy), !is.na(bc_accuracy), !is.na(elogit), !is.na(elogit_bc))

iccs <- d_summary |>
  group_by(dataset_name) |> 
  nest() |>
  mutate(icc_stimulus_acc = unlist(map(data, ~get_icc(.x, 
                                                      column = "accuracy",
                                                      object = "stimulus"))),
         icc_admin_acc = unlist(map(data, ~get_icc(.x, 
                                                   column = "accuracy",
                                                   object = "administration"))),
         icc_stimulus_bc = unlist(map(data, ~get_icc(.x, 
                                                     column = "bc_accuracy",
                                                     object = "stimulus"))),
         icc_admin_bc = unlist(map(data, ~get_icc(.x, 
                                                  column = "bc_accuracy",
                                                  object = "administration"))),
         icc_stimulus_elogit = unlist(map(data, ~get_icc(.x, 
                                                         column = "elogit",
                                                         object = "stimulus"))),
         icc_admin__elogit = unlist(map(data, ~get_icc(.x, 
                                                       column = "elogit",
                                                       object = "administration"))),
         icc_stimulus_elogitbc = unlist(map(data, ~get_icc(.x, 
                                                           column = "elogit_bc",
                                                           object = "stimulus"))),
         icc_admin_elogitbc = unlist(map(data, ~get_icc(.x, 
                                                        column = "elogit_bc",
                                                        object = "administration")))) |>
  select(-data) |>
  unnest(cols = c())
```

Let's plot these. 

```{r}
iccs_long <- iccs |>
  pivot_longer(-dataset_name, names_to = "measure", values_to = "icc") |>
  separate(measure, into = c("extra", "dimension","measure")) |>
  select(-extra) 

ggplot(iccs_long, 
       aes(x = measure, y = icc, group = dataset_name)) + 
  geom_point() +
  geom_line(alpha = .5) + 
  stat_summary(aes(group = 1), col = "red") + 
  facet_wrap(~dimension) +
  ylim(0,1) + 
  ylab("ICC") + 
  xlab("Measure")
```

# Measure 3: Reaction time

## Computing RT 
First compute reaction time. 

We need RLE data, then we use the RT helper from peekbank-shiny. 

```{r}
source("../peekbank-shiny/helpers/rt_helper.R")
```

Compute RTs, relying on the RLE workflow from the shiny app. 

```{r}
rle_data <- d_trial %>%
  filter(any(t_norm == 0), # must have data at 0
         t_norm >= 0) %>% # only pass data after 0
  group_by(administration_id, trial_id, trial_order) %>%
  summarise(lengths = rle(aoi)$lengths, 
            values = rle(aoi)$values) 

d_rt <- rle_data %>%
  group_by(administration_id, trial_id, trial_order) %>%
  nest() %>%
  mutate(data = lapply(data, get_rt)) %>%
  unnest(cols = c(data)) %>%
  left_join(d_trial %>%
              select(-t_norm, -correct, -aoi) %>%
              distinct())
```

How many trials have RTs for them?

Almost every trial makes it through the computation, but what prop do we have RTs for.

```{r}
rt_stats <- d_rt %>% 
  ungroup() %>%
  summarise(nas = mean(is.na(rt)), 
            too_fast = mean(rt < 240, na.rm=TRUE), 
            d_t = mean(shift_type == "D-T", na.rm=TRUE), 
            t_d = mean(shift_type == "T-D", na.rm=TRUE),
            other = mean(shift_type == "other", na.rm=TRUE),
            no_shift = mean(shift_type == "no shift", na.rm=TRUE))

knitr::kable(rt_stats, digits = 2)
```

## RT distribution & exclusion

Examine RT distribution.

```{r}
ggplot(d_rt, aes(x = rt)) + 
  geom_histogram()
```

Logs. 

```{r}
ggplot(d_rt, aes(x = rt)) + 
  geom_histogram() +
  scale_x_log10()
```

Probably should get rid of the RTs < 250ms or so. 

```{r}
mean(d_rt$rt<350, na.rm=TRUE)
```

Filter. 

```{r}
d_rt <- filter(d_rt, 
               !is.na(rt), 
               rt > 350)
```

Look by age.

```{r}
ggplot(d_rt, 
       aes(x = age, y = rt)) + 
  geom_point(alpha = .5) +
  geom_smooth() 
```
Add dataset to try to figure out blockiness. 

```{r}
ggplot(d_rt, 
       aes(x = age, y = rt)) + 
  geom_point(alpha = .1) +
  geom_smooth() + 
  facet_wrap(~dataset_name)
```

Histogram by dataset. 

```{r}
ggplot(d_rt, 
       aes(x = rt)) + 
  geom_histogram() +
  scale_x_log10() +
  facet_wrap(~dataset_name, scales = "free_y")
```

## RT reliabilities

Let's compute reliabilities now for D-T trials (standard approach, loses half of trials). 

```{r}
d_rt_dt <- d_rt |>
  filter(shift_type == "D-T") |>
  mutate(log_rt = log(rt)) 

rt_iccs <- d_rt_dt |>
  group_by(dataset_name) |> 
  nest() |>
  mutate(stimulus_rt = 
           unlist(map(data, ~get_icc(.x, 
                                     column = "rt",
                                     object = "stimulus"))),
         admin_rt = 
           unlist(map(data, ~get_icc(.x, 
                                     column = "rt",
                                     object = "administration"))),
         stimulus_log_rt = 
           unlist(map(data, ~get_icc(.x, 
                                     column = "log_rt",
                                     object = "stimulus"))),
         admin_log_rt = 
           unlist(map(data, ~get_icc(.x, 
                                     column = "log_rt",
                                     object = "administration")))) |>
  select(-data) |>
  unnest(cols = c())

rt_iccs_long <- rt_iccs |>
  pivot_longer(names_to = "dimension", values_to = "icc", 
               stimulus_rt:admin_log_rt) |>
  ungroup() |>
  separate(dimension, into = c("dimension","measure")) |>
  mutate(dataset_name = fct_reorder(dataset_name, icc))

```

Plot. 

```{r}
ggplot(rt_iccs_long, 
       aes(x = dataset_name, y = icc, col = measure)) +
  geom_point(position = position_dodge(width = .5)) +
  geom_line() + 
  coord_flip() + 
  facet_wrap(~dimension) 
```
Why are some ICCs zero? Let's look at Pomper SalientMe.

```{r}
ps <- d_rt |> 
  filter(dataset_name == "pomper_salientme")

ggplot(ps, 
       aes(x = target_label, y = rt)) +
  geom_jitter(alpha = .5, width = .2) + 
  stat_summary(col = "red")

ggplot(ps, 
       aes(x = administration_id, y = rt)) +
  geom_jitter(alpha = .5, width = .2) + 
  stat_summary(col = "red")
```

Now check ICCs. They are zero. 

```{r}
# disaggregated
get_icc(ps, object = "stimulus", column = "rt")
get_icc(ps, object = "administration", column = "rt")
```

Is this the repeated trial thing again?

```{r}
ps_icc <- dim_icc(ps, 
                  model = "2A", 
                  type = "agreement", 
                  unit = "average",
                  object = administration_id, 
                  rater = target_label,
                  trial = trial_id, 
                  score = rt, 
                  bootstrap = 1000)

summary(ps_icc)
```

OK, we think the issue here is that we are essentially at a correlation of zero because there is so much missing data in the kid x stimulus matrix that the overlap is too low to compute ICCs. RT is sparse because you get an RT on not that many trials. 

But why do we get fewer zeros when we subset to D-T trials? Let's dig into this. 

Pomper SalientMe shows this pattern. 

```{r}
ps

ps_dt <- ps |>
  filter(shift_type == "D-T")

get_icc(ps, object = "stimulus", column = "rt")
get_icc(ps_dt, object = "stimulus", column = "rt")
```

Let's look at the cross between subjects and trials for each. 

```{r}
ps |> 
  ungroup() |>
  select(subject_id, target_label, rt) |>
  arrange(target_label) |>
  pivot_wider(names_from = "target_label", values_from = "rt") |>
  arrange(subject_id)

ps_dt |> 
  ungroup() |>
  select(subject_id, target_label, rt) |>
  arrange(target_label) |>
  pivot_wider(names_from = "target_label", values_from = "rt") |>
  arrange(subject_id) 
```

So the D-T dataframe is sparser, but looks more consistent. Let's check out the distributions. 

```{r}
ggplot(ps, aes(x = rt)) + 
  geom_histogram() + 
  facet_wrap(~shift_type)
```
This is consistent with the idea that T-D shifts are trash in this dataset. Let's look at all data. 

```{r}
ggplot(d_rt, aes(x = rt)) + 
  geom_histogram() + 
  facet_wrap(~shift_type)
```
Looks well-supported that T-D RTs are different. I now feel comfortable moving forward with D-T only. 
Let's compare ICC from RTs to ICCs from accuracy. 

```{r}
trial_ns_acc <- bind_rows(d_summary |>
                            group_by(dataset_name, subject_id) |>
                            count() |>
                            group_by(dataset_name) |>
                            summarise(n = mean(n), 
                                      dimension = "admin"),
                          d_summary |>
                            group_by(dataset_name, target_label) |>
                            count() |>
                            group_by(dataset_name) |>
                            summarise(n = mean(n), 
                                      dimension = "stimulus"))

trial_ns_rt <- bind_rows(d_rt_dt |>
                           group_by(dataset_name, subject_id) |>
                           count() |>
                           group_by(dataset_name) |>
                           summarise(n = mean(n), 
                                     dimension = "admin"),
                         bind_rows(d_rt_dt |>
                                     group_by(dataset_name, subject_id) |>
                                     count() |>
                                     group_by(dataset_name) |>
                                     summarise(n = mean(n), 
                                               dimension = "stimulus")))

acc_rt_iccs <- bind_rows(filter(iccs_long, measure == "acc") |>
                           left_join(trial_ns_acc), 
                         rt_iccs_long |>
                           left_join(trial_ns_rt)) |>
  mutate(dataset_name = fct_reorder(as.factor(dataset_name), icc))


ggplot(acc_rt_iccs, 
       aes(x = dataset_name, y = icc, col = measure)) +
  geom_point(aes(size = n), 
             position = position_dodge(width = .5)) +
  geom_line(aes(group = measure)) + 
  facet_wrap(~dimension) +
  theme(axis.text.x=element_text(angle=-90))

acc_rt_iccs |> 
  arrange(dataset_name) |>
  mutate(icc = round(icc, digits = 2), 
         n = round(n))
```

Let's plot by N. 

```{r}
ggplot(acc_rt_iccs, 
       aes(x = n, y = icc, col = dataset_name)) + 
  geom_point() + 
  geom_smooth(aes(group = 1), method = "loess", span = 10,  se = FALSE) + 
  # scale_x_log10() +
  facet_wrap(dimension~measure, scales = "free_x") + 
  xlab("N trials per child/word") + 
  ylab("Intraclass Correlation Coefficient") + 
  ylim(0,1)

```
This is interesting! We are getting a bunch of signal about individual participants from RT, actually higher ICC than accuracies. Not so much for stimulus information, where it seems like we are doing better from accuracy. Also, as predicted the number of trials per child or per word appears to relate across datasets to the ICC (though there's lots of variance at the bottom end that presumably relates to the variation in ability across kids/variation in difficulty across words). If you choose very different words you get high reliability on that dimension (see "reliability paradoxes" idea).

```{r}
save(d_rt_dt, file= here("cached_intermediates","2_d_rt_dt.Rds"))
```

# Summary

We learned:

* use ALL THE DATA (tm) when calculating accuracies
* don't exclude zoner trials because you think they have no signal - they add signal
* baseline correction doesn't help increase reliability, though correcting for baseline effects decreases stimulus-level reliability - presumably because some stimulus-level reliability is driven by visual salience (e.g., animacy) not by true differences in word difficulty. 
* RT is surprisingly reliable as a measure of participant variation (often better than accuracy) but not as good as accuracy for characterizing stimulus variation (perhaps due to the visual salience issue). 
* log RT is not much better or much worse than regular RT. 
